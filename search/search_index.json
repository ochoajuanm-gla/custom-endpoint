{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"10 Mandamientos del Data Analysis Business Understanding : preguntas simples como \u00bfQu\u00e9? \u00bfC\u00f3mo? \u00bfCu\u00e1ndo? \u00bfFrecuencia? son vitales para entender si la tarea que tenemos que hacer persigue un objetivo \"rentable\" y a partir de ah\u00ed poder priorizarlo. Ejemplo: \"necesito que generen un reporte para mostrar las entregas hechas por las sucursales\" Preguntas: \u00bfqu\u00e9 buscan resolver con el reporte? \u00bfqu\u00e9 tipo de reporte necesitan? \u00bfqu\u00e9 m\u00e9tricas hay que mostrar? \u00bfqu\u00e9 sucursales aplican, B2B, B2C o todas? \u00bfc\u00f3mo necesitan el entregable? \u00bfcu\u00e1ndo necesitan la informaci\u00f3n? \u00bffrecuencia de actualizaci\u00f3n? Metodolog\u00eda DRY : don't repeat yourself. Consultar con el resto si lo que nos piden ya est\u00e1 resuelto y sirve para lo que nos piden. Blanquear el trabajo SIEMPRE : cuando nos mandan mails o nos cruzan en el pasillo pidi\u00e9ndonos algo, debemos pasar esos pedidos a tareas en JIRA. Ya sean en kanban o pertenezcan a alg\u00fan proyecto. Analizar, estimar y priorizar : cada tarea que hagamos, debe ser analizada para entender su complejidad y las sub-tareas que debemos llevar a cabo. Una vez hecho esto, debemos estimar en esfuerzo, entendiendo que no s\u00f3lo vamos a desarrollar, sino que tambi\u00e9n vamos a testear y documentar (lo m\u00ednimo). 4 ojos son mejor que 2: si estamos en un desarrollo y nos encontramos con un punto que no podemos resolver, es decir, investigamos un tiempo pertinente y no pudimos encontrar una soluci\u00f3n por nuestra cuenta, siempre debemos levantar la mano y pedir ayudar, ya sea a alguien de nuestro equipo o de alg\u00fan otro equipo que nos pueda ayudar. Versionar el c\u00f3digo : como buena pr\u00e1ctica de cualquier \u00e1rea de desarrollo, tener un tracking de los cambios que se hacen en el c\u00f3digo nos ayuda a todos para poder encontrar bugs que se vayan generando en el futuro y para tambi\u00e9n aplicar metodolog\u00eda estilo DevOps y hacer implementaciones por Jenkins u otra herramienta similar (utop\u00eda). Preguntar lo que no entendemos : no aplica s\u00f3lo para una tarea de desarrollo, sino tambi\u00e9n para cualquier \u00e1mbito. Si no entendemos algo de INTEGRA o de alg\u00fan sistema de ANDREANI, siempre podemos preguntar y que nos puedan responder. Eso nos fortalece la parte \"blanda\" relacionada al \"entendimiento del negocio\" y nos va a ayudar a afrontar requerimientos futuros de cualquier \u00edndole. No copiar y pegar c\u00f3digo : cuando encaremos un desarrollo en particular, al encontrarnos con c\u00f3digo que no entendemos, por m\u00e1s que funcione en otro requerimiento anterior, siempre preguntemos al autor por qu\u00e9 esa l\u00ednea se escribi\u00f3 as\u00ed y no de otra manera (por ejemplo optimizada). Cuestionar lo ya hecho : d\u00eda a d\u00eda van saliendo actualizaciones relativas a plataformas o m\u00e9todos de hacer alguna tarea de forma m\u00e1s eficiente. Es responsabilidad de todos estar al d\u00eda con las innovaciones para poder proponer su aplicaci\u00f3n en nuestra \u00e1rea. No achancharse : es algo para aplicarlo a varios aspectos de la vida, en particular en nuestra \u00e1rea tiene que ver con siempre estar movi\u00e9ndonos en referencia a leer art\u00edculos sobre tecnolog\u00eda, capacitarnos en herramientas nuevas, certificarnos en lo que nos parezca redituable. Por sobre todo lo anterior, compartir lo que encontremos y nos parezca copado para que los dem\u00e1s tambi\u00e9n est\u00e9n al tanto. De esa forma, todos estamos alineados. Reflexiones en el monte Sinai Solo se iniciar\u00e1 una actividad una vez creado un ticket en la mesa de ayuda, el trabajo debe ser mensurado en tiempo y calidad de servicio. Toda requisito fuera de un proyecto y sin asignaci\u00f3n en la mesa de ayuda, deber\u00e1 ser tratado con baja prioridad.","title":"Overview"},{"location":"Data_engineering/","text":"Data Engineering \u00b6 La ingenier\u00eda de datos es un conjunto de operaciones destinadas a crear interfaces y mecanismos para el flujo y acceso a la informaci\u00f3n. \u00bfQui\u00e9nes lo hacen? \u00b6 Se necesitan especialistas dedicados (ingenieros de datos ) para mantener los datos de modo que permanezcan disponibles y utilizables por otros. \u00bfPara qu\u00e9? \u00b6 Los ingenieros de datos configuran y operan la infraestructura de datos de la organizaci\u00f3n y la preparan para un an\u00e1lisis posterior por parte de analistas de datos y cient\u00edficos. \u00b6 Obtener los conjuntos de datos necesarios para el planteamiento del problema Desarrollar , construir y mantener arquitecturas Alinear la arquitectura con los requisitos del negocio Desarrollar el proceso del conjunto de datos Utilizar lenguajes y herramientas de programaci\u00f3n para ejecutar el conjunto de datos Encontrar el m\u00e9todo para mejorar la fiabilidad y eficiencia de los datos Utilizar grandes conjuntos de datos para resolver problemas de la empresa Importar el aprendizaje autom\u00e1tico y los m\u00e9todos estad\u00edsticos Realizar los modelos de aprendizaje autom\u00e1tico como predictivos y prescriptivos Utilizar los datos necesarios para preparar las tareas que se automatizar\u00e1n Entregar los resultados a las partes interesadas bas\u00e1ndose en los an\u00e1lisis realizados \u00bfPor qu\u00e9 es importante? \u00b6 La ingenier\u00eda de datos es importante porque permite a las empresas optimizar los datos hacia su usabilidad. Esto posibilita: - Encontrar las mejores pr\u00e1cticas para perfeccionar el ciclo de vida de desarrollo de software - Reforzar la seguridad de la informaci\u00f3n y proteger la empresa de los ciberataques - Aumentar la comprensi\u00f3n del conocimiento del dominio del negocio - Reunir los datos en un solo lugar mediante herramientas de integraci\u00f3n de datos 6 responsabilidades de un Data Engineer \u00b6 Entre las responsabilidades podemos enumerar: Mover datos entre sistemas Administrar el data warehouse Construir y administrar data pipelines Disponibilizar los datos a los usuarios finales Llevar a cabo la estrategia de datos de la compa\u00f1\u00eda Deploy de modelos ML a ambientes productivos 1) Mover datos entre sistemas \u00b6 Esta es la principal responsabilidad de un Data Engineer. Extracci\u00f3n : Extraer datos de m\u00faltiples fuentes como por ejemplo API\u2019s externas, bases de datos, archivos planos, almacenamiento en la nube (S3, Azure Storage), etc. Transformaci\u00f3n : Se trata de transformar los datos con el objetivo de filtrarlos, enriquecerlos, agregarlos, cambiar su estructura. Carga : En este paso los datos son cargados en la base final donde ser\u00e1n consumidos por otros sistemas. Esta base puede ser un data warehouse, almacenamiento en la nube, bases de datos en memoria, etc. 2) Administrar el data warehouse \u00b6 Cada vez m\u00e1s empresas est\u00e1n comenzando a utilizar data warehouses en su arquitectura de datos. Aqu\u00ed las responsabilidades de los Data Engineers son: Modelado del datawarehouse : para modelar los datos de forma tal que las consultas anal\u00edticas demoren menos tiempo. Performance del datawarehouse : para asegurarse de que las consultas se ejecuten de forma r\u00e1pida y garantizar que el warehouse pueda escalar sin sufrir un deterioro en la performance a medida que la cantidad de datos aumenta. Calidad de los datos : para asegurarse de que la calidad en los datos es la adecuada. 3) Construir y administrar Data Pipelines \u00b6 Se trata de: Mover datos entre sistemas, entre bases de datos, entre warehouses, etc Transformar: los datos entre formatos, hacer agregaciones, etc. Monitorear ca\u00f1er\u00edas de datos Administrar metadatos Algunos programas utilizados con este fin son: Airflow, Prefect, Dagster, AWS Glue, AWS Lambda, Data Factory 4) Disponibilizar los datos a los usuarios finales \u00b6 Con los datos disponibles en el data warehouse, es tiempo de disponibilizarlos a los usuarios finales. Ellos pueden ser analistas, aplicaciones, clientes externos, etc. Dependiendo del usuario final se debe configurar: Reporte/Dashboard : Son plataformas utilizadas para analizar los datos de forma gr\u00e1fica e intuitiva y algunas plataformas pueden ser: Tableau, Metabase, Superset, Power BI. Permisos de acceso : para una tabla hay que generar los permisos para el acceso de usuarios y aplicaciones. Endpoints (APIs) : algunas aplicaciones/clientes externos quiz\u00e1s necesiten acceso mediante una API para consultar la informaci\u00f3n. Volcado de datos para clientes : Algunos clientes quiz\u00e1s requieran extracciones espec\u00edficas de informaci\u00f3n. En esos casos, el Data Engineer deber\u00e1 generar los pipelines necesarios para disponibilizar esas extracciones. 5) Llevar a cabo la estrategia de datos de la compa\u00f1\u00eda \u00b6 Esto incluye: Decidir qu\u00e9 datos recolectar, c\u00f3mo recolectarlos y c\u00f3mo guardarlos de forma segura Liderar la evoluci\u00f3n de la arquitectura de datos para satisfacer nuevas necesidades de informaci\u00f3n Educar a los usuarios finales sobre c\u00f3mo usar los datos de forma efectiva Decidir qu\u00e9 datos compartir con usuarios finales 6) Deploy de modelos ML a ambientes productivos \u00b6 Los cient\u00edficos de datos construyen modelos que predicen de forma acertada el comportamiento de determinados procesos de negocio. El Data Engineer podr\u00e1 optimizarlos para utilizarlos en un ambiente productivo.","title":"Data Engineering"},{"location":"Data_engineering/#data-engineering","text":"La ingenier\u00eda de datos es un conjunto de operaciones destinadas a crear interfaces y mecanismos para el flujo y acceso a la informaci\u00f3n.","title":"Data Engineering"},{"location":"Data_engineering/#quienes-lo-hacen","text":"Se necesitan especialistas dedicados (ingenieros de datos ) para mantener los datos de modo que permanezcan disponibles y utilizables por otros.","title":"\u00bfQui\u00e9nes lo hacen?"},{"location":"Data_engineering/#para-que","text":"Los ingenieros de datos configuran y operan la infraestructura de datos de la organizaci\u00f3n y la preparan para un an\u00e1lisis posterior por parte de analistas de datos y cient\u00edficos.","title":"\u00bfPara qu\u00e9?"},{"location":"Data_engineering/#_1","text":"Obtener los conjuntos de datos necesarios para el planteamiento del problema Desarrollar , construir y mantener arquitecturas Alinear la arquitectura con los requisitos del negocio Desarrollar el proceso del conjunto de datos Utilizar lenguajes y herramientas de programaci\u00f3n para ejecutar el conjunto de datos Encontrar el m\u00e9todo para mejorar la fiabilidad y eficiencia de los datos Utilizar grandes conjuntos de datos para resolver problemas de la empresa Importar el aprendizaje autom\u00e1tico y los m\u00e9todos estad\u00edsticos Realizar los modelos de aprendizaje autom\u00e1tico como predictivos y prescriptivos Utilizar los datos necesarios para preparar las tareas que se automatizar\u00e1n Entregar los resultados a las partes interesadas bas\u00e1ndose en los an\u00e1lisis realizados","title":""},{"location":"Data_engineering/#por-que-es-importante","text":"La ingenier\u00eda de datos es importante porque permite a las empresas optimizar los datos hacia su usabilidad. Esto posibilita: - Encontrar las mejores pr\u00e1cticas para perfeccionar el ciclo de vida de desarrollo de software - Reforzar la seguridad de la informaci\u00f3n y proteger la empresa de los ciberataques - Aumentar la comprensi\u00f3n del conocimiento del dominio del negocio - Reunir los datos en un solo lugar mediante herramientas de integraci\u00f3n de datos","title":"\u00bfPor qu\u00e9 es importante?"},{"location":"Data_engineering/#6-responsabilidades-de-un-data-engineer","text":"Entre las responsabilidades podemos enumerar: Mover datos entre sistemas Administrar el data warehouse Construir y administrar data pipelines Disponibilizar los datos a los usuarios finales Llevar a cabo la estrategia de datos de la compa\u00f1\u00eda Deploy de modelos ML a ambientes productivos","title":"6 responsabilidades de un Data Engineer"},{"location":"Data_engineering/#1-mover-datos-entre-sistemas","text":"Esta es la principal responsabilidad de un Data Engineer. Extracci\u00f3n : Extraer datos de m\u00faltiples fuentes como por ejemplo API\u2019s externas, bases de datos, archivos planos, almacenamiento en la nube (S3, Azure Storage), etc. Transformaci\u00f3n : Se trata de transformar los datos con el objetivo de filtrarlos, enriquecerlos, agregarlos, cambiar su estructura. Carga : En este paso los datos son cargados en la base final donde ser\u00e1n consumidos por otros sistemas. Esta base puede ser un data warehouse, almacenamiento en la nube, bases de datos en memoria, etc.","title":"1) Mover datos entre sistemas"},{"location":"Data_engineering/#2-administrar-el-data-warehouse","text":"Cada vez m\u00e1s empresas est\u00e1n comenzando a utilizar data warehouses en su arquitectura de datos. Aqu\u00ed las responsabilidades de los Data Engineers son: Modelado del datawarehouse : para modelar los datos de forma tal que las consultas anal\u00edticas demoren menos tiempo. Performance del datawarehouse : para asegurarse de que las consultas se ejecuten de forma r\u00e1pida y garantizar que el warehouse pueda escalar sin sufrir un deterioro en la performance a medida que la cantidad de datos aumenta. Calidad de los datos : para asegurarse de que la calidad en los datos es la adecuada.","title":"2) Administrar el data warehouse"},{"location":"Data_engineering/#3-construir-y-administrar-data-pipelines","text":"Se trata de: Mover datos entre sistemas, entre bases de datos, entre warehouses, etc Transformar: los datos entre formatos, hacer agregaciones, etc. Monitorear ca\u00f1er\u00edas de datos Administrar metadatos Algunos programas utilizados con este fin son: Airflow, Prefect, Dagster, AWS Glue, AWS Lambda, Data Factory","title":"3) Construir y administrar Data Pipelines"},{"location":"Data_engineering/#4-disponibilizar-los-datos-a-los-usuarios-finales","text":"Con los datos disponibles en el data warehouse, es tiempo de disponibilizarlos a los usuarios finales. Ellos pueden ser analistas, aplicaciones, clientes externos, etc. Dependiendo del usuario final se debe configurar: Reporte/Dashboard : Son plataformas utilizadas para analizar los datos de forma gr\u00e1fica e intuitiva y algunas plataformas pueden ser: Tableau, Metabase, Superset, Power BI. Permisos de acceso : para una tabla hay que generar los permisos para el acceso de usuarios y aplicaciones. Endpoints (APIs) : algunas aplicaciones/clientes externos quiz\u00e1s necesiten acceso mediante una API para consultar la informaci\u00f3n. Volcado de datos para clientes : Algunos clientes quiz\u00e1s requieran extracciones espec\u00edficas de informaci\u00f3n. En esos casos, el Data Engineer deber\u00e1 generar los pipelines necesarios para disponibilizar esas extracciones.","title":"4) Disponibilizar los datos a los usuarios finales"},{"location":"Data_engineering/#5-llevar-a-cabo-la-estrategia-de-datos-de-la-compania","text":"Esto incluye: Decidir qu\u00e9 datos recolectar, c\u00f3mo recolectarlos y c\u00f3mo guardarlos de forma segura Liderar la evoluci\u00f3n de la arquitectura de datos para satisfacer nuevas necesidades de informaci\u00f3n Educar a los usuarios finales sobre c\u00f3mo usar los datos de forma efectiva Decidir qu\u00e9 datos compartir con usuarios finales","title":"5) Llevar a cabo la estrategia de datos de la compa\u00f1\u00eda"},{"location":"Data_engineering/#6-deploy-de-modelos-ml-a-ambientes-productivos","text":"Los cient\u00edficos de datos construyen modelos que predicen de forma acertada el comportamiento de determinados procesos de negocio. El Data Engineer podr\u00e1 optimizarlos para utilizarlos en un ambiente productivo.","title":"6) Deploy de modelos ML a ambientes productivos"},{"location":"DevOps/","text":"","title":"DevOps"},{"location":"EN_CONSTRUCCI%C3%93N/","text":"P\u00c1GINA EN CONSTRUCCI\u00d3N \ud83d\udc77\u200d\u2642\ufe0f \u00b6 \u26a0\ufe0fProximamente habilitada","title":"DevOps"},{"location":"EN_CONSTRUCCI%C3%93N/#pagina-en-construccion","text":"\u26a0\ufe0fProximamente habilitada","title":"P\u00c1GINA EN CONSTRUCCI\u00d3N \ud83d\udc77\u200d\u2642\ufe0f"},{"location":"airflow/","text":"Arquitectura en Airflow \u00b6 Ejecutores \u00b6 Airflow se compone de un servidor web que sirve la API, la interfaz de usuario y gestiona las peticiones y de un planificador (Scheduler) encargado de interpretar, ejecutar y monitorizar las tareas definidas en los DAGs. Este planificador contiene un ejecutor, encargado de lanzar los workers y repartir en ellos las tareas. Airflow tambi\u00e9n tiene una base de datos a modo de backend encargada de almacenar los metadatos, usuarios y ejecuciones. Por defecto usa Sqlite pero podemos usar otra base de datos en entornos de producci\u00f3n, como MySQL. Existen varias formas de desplegar Apache Airflow , con m\u00faltiples arquitecturas para sus ejecutores: Local, Sequential, Celery, Dask, Mesos o Kubernetes . Tambi\u00e9n se puede usar con servicios en la nube de Azure , AWS o Google Cloud. Los m\u00e1s usados \u00b6 Single-Node Executors : Los ejecutores de este tipo solo permiten la ejecuci\u00f3n de tareas en un nodo o worker, que es el mismo host en el que se encuentra el Scheduler. No debemos considerar estos ejecutores para sistemas en producci\u00f3n ya que no son escalables y son puntos \u00fanicos de fallo. Sequential : Este tipo de ejecutor secuencial se usa para debugging de DAGs y es compatible con SQLite a trav\u00e9s de una \u00fanica conexi\u00f3n de escritura. Ejecuta solo una tarea en cada instante. Local : Ejecuta las tareas en paralelo. Es el entorno m\u00ednimo que se podr\u00eda considerar para una aplicaci\u00f3n real. Tambi\u00e9n es compatible con SQLite Cluster Executors : Para gestionar los recursos, Apache Airflow necesita una asignaci\u00f3n fija de workers sobre los que distribuir la carga de trabajo. Celery : Ejecuta tareas en paralelo en varios nodos separados, por lo que permite escalar el sistema de forma horizontal y vertical. Este tipo de ejecutor requiere desplegar un gestor de colas como puede ser RabbitMQ . Adem\u00e1s, se puede asignar cada tarea a una cola de procesamiento. En caso de fallo en alg\u00fan worker podr\u00eda desplegar uno nuevo. Dask : Ejecuta cada una de las tareas en varios nodos dividiendo la carga de trabajo, por lo que puede conseguir un rendimiento mayor a Celery si se tiene en cuenta la localidad del dato (el trabajo ejecuta en el nodo que contenga el dato). Kubernetes : Ejecuta cada tarea en un pod de Kubernetes, desplegando nuevos pods seg\u00fan la demanda de recursos. De esta forma, se consigue aprovechar los recursos de manera m\u00e1s eficiente.","title":"Aquitectura"},{"location":"airflow/#arquitectura-en-airflow","text":"","title":"Arquitectura en Airflow"},{"location":"airflow/#ejecutores","text":"Airflow se compone de un servidor web que sirve la API, la interfaz de usuario y gestiona las peticiones y de un planificador (Scheduler) encargado de interpretar, ejecutar y monitorizar las tareas definidas en los DAGs. Este planificador contiene un ejecutor, encargado de lanzar los workers y repartir en ellos las tareas. Airflow tambi\u00e9n tiene una base de datos a modo de backend encargada de almacenar los metadatos, usuarios y ejecuciones. Por defecto usa Sqlite pero podemos usar otra base de datos en entornos de producci\u00f3n, como MySQL. Existen varias formas de desplegar Apache Airflow , con m\u00faltiples arquitecturas para sus ejecutores: Local, Sequential, Celery, Dask, Mesos o Kubernetes . Tambi\u00e9n se puede usar con servicios en la nube de Azure , AWS o Google Cloud.","title":"Ejecutores"},{"location":"airflow/#los-mas-usados","text":"Single-Node Executors : Los ejecutores de este tipo solo permiten la ejecuci\u00f3n de tareas en un nodo o worker, que es el mismo host en el que se encuentra el Scheduler. No debemos considerar estos ejecutores para sistemas en producci\u00f3n ya que no son escalables y son puntos \u00fanicos de fallo. Sequential : Este tipo de ejecutor secuencial se usa para debugging de DAGs y es compatible con SQLite a trav\u00e9s de una \u00fanica conexi\u00f3n de escritura. Ejecuta solo una tarea en cada instante. Local : Ejecuta las tareas en paralelo. Es el entorno m\u00ednimo que se podr\u00eda considerar para una aplicaci\u00f3n real. Tambi\u00e9n es compatible con SQLite Cluster Executors : Para gestionar los recursos, Apache Airflow necesita una asignaci\u00f3n fija de workers sobre los que distribuir la carga de trabajo. Celery : Ejecuta tareas en paralelo en varios nodos separados, por lo que permite escalar el sistema de forma horizontal y vertical. Este tipo de ejecutor requiere desplegar un gestor de colas como puede ser RabbitMQ . Adem\u00e1s, se puede asignar cada tarea a una cola de procesamiento. En caso de fallo en alg\u00fan worker podr\u00eda desplegar uno nuevo. Dask : Ejecuta cada una de las tareas en varios nodos dividiendo la carga de trabajo, por lo que puede conseguir un rendimiento mayor a Celery si se tiene en cuenta la localidad del dato (el trabajo ejecuta en el nodo que contenga el dato). Kubernetes : Ejecuta cada tarea en un pod de Kubernetes, desplegando nuevos pods seg\u00fan la demanda de recursos. De esta forma, se consigue aprovechar los recursos de manera m\u00e1s eficiente.","title":"Los m\u00e1s usados"},{"location":"arquitectura/","text":"Arquitectura \u00b6","title":"Aquitectura"},{"location":"arquitectura/#arquitectura","text":"","title":"Arquitectura"},{"location":"casos_uso/","text":"Casos de Uso \u00b6 Apache Airflow es una herramienta de coordinaci\u00f3n de jobs realizados por otros servicios. Resulta muy \u00fatil para gestionar los workflows en Data Warehouses y en pipelines de Machine Learning . Aunque Airflow no es una herramienta ETL ayuda a gestionar y a monitorizar este tipo de procesos. Adem\u00e1s, Airflow integra una interfaz de usuario sencilla, una herramienta CLI que proporciona control del estado de ejecuci\u00f3n de todo el sistema. Adem\u00e1s, podemos usar Airflow para orquestar testing autom\u00e1tico de componentes, backups y generaci\u00f3n de m\u00e9tricas y reportes.","title":"Casos de uso"},{"location":"casos_uso/#casos-de-uso","text":"Apache Airflow es una herramienta de coordinaci\u00f3n de jobs realizados por otros servicios. Resulta muy \u00fatil para gestionar los workflows en Data Warehouses y en pipelines de Machine Learning . Aunque Airflow no es una herramienta ETL ayuda a gestionar y a monitorizar este tipo de procesos. Adem\u00e1s, Airflow integra una interfaz de usuario sencilla, una herramienta CLI que proporciona control del estado de ejecuci\u00f3n de todo el sistema. Adem\u00e1s, podemos usar Airflow para orquestar testing autom\u00e1tico de componentes, backups y generaci\u00f3n de m\u00e9tricas y reportes.","title":"Casos de Uso"},{"location":"concepts_databricks/","text":"Conceptos \u00b6 Algunos conceptos son generales para Azure Databricks y otros son espec\u00edficos del entorno de Azure Databricks basado en personas que estamos usando: Databricks Data Science & Engineering Databricks Machine Learning Databricks SQL Workspaces \u00b6 En Azure Databricks, workspace tiene dos significados: Una implementaci\u00f3n de Azure Databricks en la nube que funciona como el entorno unificado que se usa en el equipo Data para acceder a todos los activos de Databricks. La interfaz de usuario para los entornos basados en personas Databricks Data Science & Engineering y Databricks Machine Learning. Esto es lo opuesto al entorno SQL de Databricks. Cuando hablamos del \" workspace browser\", por ejemplo, nos referimos a la interfaz de usuario que le permite explorar notebooks, librerias y otros archivos en los entornos de Data Science & Engineering y Databricks Machine Learning, una interfaz de usuario que no forma parte del entorno de Databricks SQL. Aunque todos est\u00e1n incluidos en el workspace implementado de Azure Databricks.","title":"Conceptos"},{"location":"concepts_databricks/#conceptos","text":"Algunos conceptos son generales para Azure Databricks y otros son espec\u00edficos del entorno de Azure Databricks basado en personas que estamos usando: Databricks Data Science & Engineering Databricks Machine Learning Databricks SQL","title":"Conceptos"},{"location":"concepts_databricks/#workspaces","text":"En Azure Databricks, workspace tiene dos significados: Una implementaci\u00f3n de Azure Databricks en la nube que funciona como el entorno unificado que se usa en el equipo Data para acceder a todos los activos de Databricks. La interfaz de usuario para los entornos basados en personas Databricks Data Science & Engineering y Databricks Machine Learning. Esto es lo opuesto al entorno SQL de Databricks. Cuando hablamos del \" workspace browser\", por ejemplo, nos referimos a la interfaz de usuario que le permite explorar notebooks, librerias y otros archivos en los entornos de Data Science & Engineering y Databricks Machine Learning, una interfaz de usuario que no forma parte del entorno de Databricks SQL. Aunque todos est\u00e1n incluidos en el workspace implementado de Azure Databricks.","title":"Workspaces"},{"location":"dag/","text":"DAGs en Apache Airflow \u00b6 En Airflow , se trabaja con DAGs (Directed Acyclic Graphs) . Son colecciones de tareas o de trabajos a ejecutar conectados mediante relaciones y dependencias. Son la representaci\u00f3n de los workflows. Los grafos deben cumplir dos condiciones: ser dirigidos y ac\u00edclicos: 23Q2 Dirigidos : Las relaciones entre los nodos tienen solo un sentido. Ac\u00edclicos : No pueden formar ciclos, es decir, la ejecuci\u00f3n no puede volver a un nodo que ya ha ejecutado. Cada una de las tareas del DAG representada como un nodo, se describe con un operador y generalmente es at\u00f3mica. Existen operadores predefinidos, y es posible extender y crear nuevos operadores si fueran necesarios. Por ejemplo, BashOperator se encarga de ejecutar un comando Bash mientras que PythonOperator se encarga de ejecutar una funci\u00f3n de Python . Ejemplo de flujo en Apache Airflow En este ejemplo, observamos que es posible definir ramas o branches en un grafo. En cada divisi\u00f3n del flujo solo ejecutan las tareas definidas en una de las ramas, dependiendo de una condici\u00f3n evaluada anteriormente. Al no ser posible definir una rama sin ninguna tarea, para representar ramas sin acciones se usan tareas vac\u00edas llamadas dummy tasks . Por ejemplo, un workflow o pipeline sencillo podr\u00eda contener las siguientes tareas: Descargar datos de una base de datos MySQL Enviar los datos a un cl\u00faster de Azure Realizar transformaciones sobre los datos con Databricks Generar un mensaje de terminaci\u00f3n Entre los operadores m\u00e1s usados se encuentran los siguientes: Bash Operator : Permite ejecutar scripts en Bash, aunque es posible modificarlo. Database Operator : Nos permite interactuar con bases de datos. Se usan al obtener datos de una base de datos mediante consultas SQL e informaci\u00f3n de autenticaci\u00f3n. Es compatible con bases de datos populares como MySQL, Postgres, Sqlite o con JDBC. Python Operator : Ejecuta scripts en Python y operaciones creadas para el DAG. Sensor Operator : Est\u00e1 a la espera de detectar modificaciones en sistemas externos como ficheros o fuentes de datos. Email Operator : Este operador permite enviar un email a modo de notificaci\u00f3n. HTTP Operator : Permite usar una API HTTP que necesite autenticaci\u00f3n. La creaci\u00f3n de DAGs se realiza a trav\u00e9s de c\u00f3digo Python . Realmente, el fichero python es un fichero de configuraci\u00f3n que especifica la estructura del DAG con c\u00f3digo. El prop\u00f3sito de este script es definir el objeto DAG por lo que debe evaluarse r\u00e1pidamente y por tanto no es el lugar para realizar procesamiento de datos. La interfaz gr\u00e1fica permite monitorizar el estado de ejecuci\u00f3n de las tareas, pero no permite crear nuevos flujos. Caracter\u00edsticas \u00b6 Cuando una tarea se ejecuta en Airflow , se denomina instancia, y tiene un tiempo asociado al momento de ejecuci\u00f3n. Adem\u00e1s, incorporan un atributo que describe su estado de ejecuci\u00f3n: Running, Failed, Success , etc. La combinaci\u00f3n de estas instancias de tareas genera un workflow o un flujo de trabajo. Para establecer los tiempos m\u00e1ximos de ejecuci\u00f3n para cada tarea, Airflow trabaja con SLAs (Service Level Agreements). Cuando el SLA no se cumple para alguna tarea, se puede informar al usuario o administrador del sistema. Aunque las tareas de Airflow pueden intercambiar metadatos, no se deben encargar de mover datos entre ellas. Las tareas definidas en el script ejecutan en un contexto diferente en los ejecutores y en diferentes instantes de tiempo. Esto significa que no se puede usar el script para comunicaci\u00f3n entre tareas. Aun as\u00ed, es posible gestionar el paso de mensajes y compartir el estado entre las tareas que ejecutan. Para ello se usa XCom (Cross-communication) y tambi\u00e9n se pueden definir variables en formato clave-valor. Airflow tambi\u00e9n proporciona interfaces de comunicaci\u00f3n llamadas Hooks para conectar con otras plataformas y bases de datos externas como pueden ser HDFS o Apache Hive . Los conectores facilitan la creaci\u00f3n de nuevas tareas independientemente del origen y del destino de los datos. Adem\u00e1s, Airflow permite un control visual del estado de cada paso, facilitando la trazabilidad y la localizaci\u00f3n de errores conservando un hist\u00f3rico de fallos para detectar y prevenir problemas. Cuando una tarea falla, podemos configurar el sistema para que se reintente, o bien definir actuaciones en funci\u00f3n del fallo que haya ocurrido.","title":"DAG"},{"location":"dag/#dags-en-apache-airflow","text":"En Airflow , se trabaja con DAGs (Directed Acyclic Graphs) . Son colecciones de tareas o de trabajos a ejecutar conectados mediante relaciones y dependencias. Son la representaci\u00f3n de los workflows. Los grafos deben cumplir dos condiciones: ser dirigidos y ac\u00edclicos: 23Q2 Dirigidos : Las relaciones entre los nodos tienen solo un sentido. Ac\u00edclicos : No pueden formar ciclos, es decir, la ejecuci\u00f3n no puede volver a un nodo que ya ha ejecutado. Cada una de las tareas del DAG representada como un nodo, se describe con un operador y generalmente es at\u00f3mica. Existen operadores predefinidos, y es posible extender y crear nuevos operadores si fueran necesarios. Por ejemplo, BashOperator se encarga de ejecutar un comando Bash mientras que PythonOperator se encarga de ejecutar una funci\u00f3n de Python . Ejemplo de flujo en Apache Airflow En este ejemplo, observamos que es posible definir ramas o branches en un grafo. En cada divisi\u00f3n del flujo solo ejecutan las tareas definidas en una de las ramas, dependiendo de una condici\u00f3n evaluada anteriormente. Al no ser posible definir una rama sin ninguna tarea, para representar ramas sin acciones se usan tareas vac\u00edas llamadas dummy tasks . Por ejemplo, un workflow o pipeline sencillo podr\u00eda contener las siguientes tareas: Descargar datos de una base de datos MySQL Enviar los datos a un cl\u00faster de Azure Realizar transformaciones sobre los datos con Databricks Generar un mensaje de terminaci\u00f3n Entre los operadores m\u00e1s usados se encuentran los siguientes: Bash Operator : Permite ejecutar scripts en Bash, aunque es posible modificarlo. Database Operator : Nos permite interactuar con bases de datos. Se usan al obtener datos de una base de datos mediante consultas SQL e informaci\u00f3n de autenticaci\u00f3n. Es compatible con bases de datos populares como MySQL, Postgres, Sqlite o con JDBC. Python Operator : Ejecuta scripts en Python y operaciones creadas para el DAG. Sensor Operator : Est\u00e1 a la espera de detectar modificaciones en sistemas externos como ficheros o fuentes de datos. Email Operator : Este operador permite enviar un email a modo de notificaci\u00f3n. HTTP Operator : Permite usar una API HTTP que necesite autenticaci\u00f3n. La creaci\u00f3n de DAGs se realiza a trav\u00e9s de c\u00f3digo Python . Realmente, el fichero python es un fichero de configuraci\u00f3n que especifica la estructura del DAG con c\u00f3digo. El prop\u00f3sito de este script es definir el objeto DAG por lo que debe evaluarse r\u00e1pidamente y por tanto no es el lugar para realizar procesamiento de datos. La interfaz gr\u00e1fica permite monitorizar el estado de ejecuci\u00f3n de las tareas, pero no permite crear nuevos flujos.","title":"DAGs en Apache Airflow"},{"location":"dag/#caracteristicas","text":"Cuando una tarea se ejecuta en Airflow , se denomina instancia, y tiene un tiempo asociado al momento de ejecuci\u00f3n. Adem\u00e1s, incorporan un atributo que describe su estado de ejecuci\u00f3n: Running, Failed, Success , etc. La combinaci\u00f3n de estas instancias de tareas genera un workflow o un flujo de trabajo. Para establecer los tiempos m\u00e1ximos de ejecuci\u00f3n para cada tarea, Airflow trabaja con SLAs (Service Level Agreements). Cuando el SLA no se cumple para alguna tarea, se puede informar al usuario o administrador del sistema. Aunque las tareas de Airflow pueden intercambiar metadatos, no se deben encargar de mover datos entre ellas. Las tareas definidas en el script ejecutan en un contexto diferente en los ejecutores y en diferentes instantes de tiempo. Esto significa que no se puede usar el script para comunicaci\u00f3n entre tareas. Aun as\u00ed, es posible gestionar el paso de mensajes y compartir el estado entre las tareas que ejecutan. Para ello se usa XCom (Cross-communication) y tambi\u00e9n se pueden definir variables en formato clave-valor. Airflow tambi\u00e9n proporciona interfaces de comunicaci\u00f3n llamadas Hooks para conectar con otras plataformas y bases de datos externas como pueden ser HDFS o Apache Hive . Los conectores facilitan la creaci\u00f3n de nuevas tareas independientemente del origen y del destino de los datos. Adem\u00e1s, Airflow permite un control visual del estado de cada paso, facilitando la trazabilidad y la localizaci\u00f3n de errores conservando un hist\u00f3rico de fallos para detectar y prevenir problemas. Cuando una tarea falla, podemos configurar el sistema para que se reintente, o bien definir actuaciones en funci\u00f3n del fallo que haya ocurrido.","title":"Caracter\u00edsticas"},{"location":"databricks_datascience%26engineering/","text":"Databricks Data Science & Engineering \u00b6 Es el entorno cl\u00e1sico de Azure Databricks para la colaboraci\u00f3n entre cient\u00edficos de datos, ingenieros de datos y analistas de datos. Workspace \u00b6 Organiza los objetos (notebooks, libraries, dashboards, y experiments) en carpetas y proporciona acceso a los objetos de datos y los recursos de c\u00e1lculo. Objetos contenidos en las carpetas del workspace de Azure Databricks. Notebook Dashboard Library Repo Experiment \ud83d\udcdd Nota: As\u00ed se ver\u00eda la interfaz en Databricks. Data management \u00b6 En esta secci\u00f3n se describen los objetos que contiene los datos sobre los que se realizan an\u00e1lisis y que alimentan los algoritmos de aprendizaje autom\u00e1tico. Databricks File System (DBFS) Database Table Metastore Computation management \u00b6 En esta secci\u00f3n se describen los conceptos que debe conocer para ejecutar c\u00e1lculos en Azure Databricks. Cluster Pool Databricks runtime Job","title":"Databricks Data Sciente & Engineering"},{"location":"databricks_datascience%26engineering/#databricks-data-science-engineering","text":"Es el entorno cl\u00e1sico de Azure Databricks para la colaboraci\u00f3n entre cient\u00edficos de datos, ingenieros de datos y analistas de datos.","title":"Databricks Data Science &amp; Engineering"},{"location":"databricks_datascience%26engineering/#workspace","text":"Organiza los objetos (notebooks, libraries, dashboards, y experiments) en carpetas y proporciona acceso a los objetos de datos y los recursos de c\u00e1lculo. Objetos contenidos en las carpetas del workspace de Azure Databricks. Notebook Dashboard Library Repo Experiment \ud83d\udcdd Nota: As\u00ed se ver\u00eda la interfaz en Databricks.","title":"Workspace"},{"location":"databricks_datascience%26engineering/#data-management","text":"En esta secci\u00f3n se describen los objetos que contiene los datos sobre los que se realizan an\u00e1lisis y que alimentan los algoritmos de aprendizaje autom\u00e1tico. Databricks File System (DBFS) Database Table Metastore","title":"Data management"},{"location":"databricks_datascience%26engineering/#computation-management","text":"En esta secci\u00f3n se describen los conceptos que debe conocer para ejecutar c\u00e1lculos en Azure Databricks. Cluster Pool Databricks runtime Job","title":"Computation management"},{"location":"databricks_integrations/","text":"Databricks integrations \u00b6 Databricks se integra con: Data sources : Databricks puede leer y escribir datos en diversos formatos de datos, como CSV, Delta Lake ,JSON, Parquet, XML y otros formatos, as\u00ed como proveedores de almacenamiento de datos como Azure Data Lake Storage, Google BigQuery y Cloud Storage, Snowflake y otros proveedores Developer tools : Databricks admite varias herramientas de desarrollo, como Data Yap, IntelliJ, PyCharm, Visual Studio Code, etc., que permiten trabajar con datos a trav\u00e9s de cl\u00fasteres de Azure Databricks y Databricks SQL endpoints mediante la escritura de c\u00f3digo. Partner solutions : Databricks ha validado las integraciones con varios productos de terceros, como Fivetran, Power BI, Tableau y otros, que permiten trabajar con datos a trav\u00e9s de cl\u00fasteres de Azure Databricks y puntos de conexi\u00f3n de SQL, en muchos casos con baja o nula experiencia en c\u00f3digo. Estas soluciones permiten escenarios comunes, como la ingesta de datos, la preparaci\u00f3n y transformaci\u00f3n de datos, business intelligence (BI) y el aprendizaje autom\u00e1tico. Databricks tambi\u00e9n proporciona Partner Connect , una interfaz de usuario que permite que algunas de estas soluciones validadas se integren de forma m\u00e1s r\u00e1pida y sencilla con los cl\u00fasteres de Azure Databricks y SQL endpoints.","title":"Databricks integrations"},{"location":"databricks_integrations/#databricks-integrations","text":"Databricks se integra con: Data sources : Databricks puede leer y escribir datos en diversos formatos de datos, como CSV, Delta Lake ,JSON, Parquet, XML y otros formatos, as\u00ed como proveedores de almacenamiento de datos como Azure Data Lake Storage, Google BigQuery y Cloud Storage, Snowflake y otros proveedores Developer tools : Databricks admite varias herramientas de desarrollo, como Data Yap, IntelliJ, PyCharm, Visual Studio Code, etc., que permiten trabajar con datos a trav\u00e9s de cl\u00fasteres de Azure Databricks y Databricks SQL endpoints mediante la escritura de c\u00f3digo. Partner solutions : Databricks ha validado las integraciones con varios productos de terceros, como Fivetran, Power BI, Tableau y otros, que permiten trabajar con datos a trav\u00e9s de cl\u00fasteres de Azure Databricks y puntos de conexi\u00f3n de SQL, en muchos casos con baja o nula experiencia en c\u00f3digo. Estas soluciones permiten escenarios comunes, como la ingesta de datos, la preparaci\u00f3n y transformaci\u00f3n de datos, business intelligence (BI) y el aprendizaje autom\u00e1tico. Databricks tambi\u00e9n proporciona Partner Connect , una interfaz de usuario que permite que algunas de estas soluciones validadas se integren de forma m\u00e1s r\u00e1pida y sencilla con los cl\u00fasteres de Azure Databricks y SQL endpoints.","title":"Databricks integrations"},{"location":"databricks_machinelearning/","text":"Databricks Machine Learning \u00b6 Un entorno de aprendizaje autom\u00e1tico integrado de un extremo a otro que incorpora servicios administrados para el seguimiento de experimentos, entrenamientos de modelos, el desarrollo y la administraci\u00f3n de features de modelos. \ud83d\udcdd Nota: As\u00ed se ver\u00eda la interfaz en Databricks.","title":"Databricks Machine Learning"},{"location":"databricks_machinelearning/#databricks-machine-learning","text":"Un entorno de aprendizaje autom\u00e1tico integrado de un extremo a otro que incorpora servicios administrados para el seguimiento de experimentos, entrenamientos de modelos, el desarrollo y la administraci\u00f3n de features de modelos. \ud83d\udcdd Nota: As\u00ed se ver\u00eda la interfaz en Databricks.","title":"Databricks Machine Learning"},{"location":"databricks_sql/","text":"Databricks SQL \u00b6 Dirigido a analistas de datos que trabajan principalmente con consultas SQL y herramientas de BI. Proporciona un entorno intuitivo para ejecutar consultas ad-hoc y crear paneles sobre los datos almacenados en su lago de datos. \ud83d\udcdd Nota: As\u00ed se ver\u00eda la interfaz en Databricks. Data management \u00b6 Visualization Dashboard Alert Computation management \u00b6 Query ClusSQL endpointter Query history","title":"Databricks SQL"},{"location":"databricks_sql/#databricks-sql","text":"Dirigido a analistas de datos que trabajan principalmente con consultas SQL y herramientas de BI. Proporciona un entorno intuitivo para ejecutar consultas ad-hoc y crear paneles sobre los datos almacenados en su lago de datos. \ud83d\udcdd Nota: As\u00ed se ver\u00eda la interfaz en Databricks.","title":"Databricks SQL"},{"location":"databricks_sql/#data-management","text":"Visualization Dashboard Alert","title":"Data management"},{"location":"databricks_sql/#computation-management","text":"Query ClusSQL endpointter Query history","title":"Computation management"},{"location":"glosario_de_terminos/","text":"Glosario de T\u00e9rminos \u00b6 Transporte y Distribuci\u00f3n \u00b6 TMS : sistema operativo que se utiliza dependiendo la unidad de negocio. Para \"B2C\" se utiliza INTEGRA y para \"B2B\" se utiliza ALERTRAN. B2C : abreviatura de business to consumer (negocio a consumidor final). Tambi\u00e9n incluye C2C donde un consumidor final le realiza un env\u00edo a otro consumidor final. Cuando decimos consumidor final nos referimos a una persona f\u00edsica. Hace referencia generalmente a toda la parte de \"correo\" donde se transmiten env\u00edos a nivel de \"bulto\" entre ANDREANI y las personas f\u00edsicas B2B : abreviatura de business to business (negocio a negocio). Hace referencia generalmente a toda la parte de \"log\u00edstica\" donde se transmiten env\u00edos a nivel de \"pallet\" entre clientes y ANDREANI o mismo entre plantas internas de ANDREANI Expedici\u00f3n : para el \u00e1mbito \"B2C\" se utiliza cuando un env\u00edo sale desde una planta/sucursal ANDREANI hacia otra planta/sucursal de distribuci\u00f3n. Para el \u00e1mbito \"B2B\" se denomina expedici\u00f3n a todo env\u00edo existente en el TMS donde es un conjunto de bultos que se transmiten entre plantas o centros de distribuci\u00f3n desde o para ANDREANI y sus clientes. Recepci\u00f3n : para el \u00e1mbito \"B2C\" se utiliza cuando una sucursal de distribuci\u00f3n recibe un env\u00edo proveniente de una planta o centro de distribuci\u00f3n para ser clasificado y entrar en fase de distribuci\u00f3n o custodia (seg\u00fan contrato). Pallet/Colectora/Caja/Bolsa : contenedores de bultos Aforo : medidas y peso de un bulto realizadas por balanza o sorter Custodia : para el \u00e1mbito \"B2C\" el ciclo de custodia se inicia cuando un env\u00edo llega a una sucursal y no debe distribuirse al domicilio. Puede ser por 2 motivos: por contrato (nunca se distribuye) o porque se agotaron las instancias de visitas (hasta 3 seg\u00fan contrato) Traza : cualquier env\u00edo, ya sea en el \u00e1mbito \"B2C\" o \"B2B\", va sufriendo distintas actualizaciones de estado desde que nace hasta que muere (se entrega. se devuelve al cliente, etc). Por cada cambio de estado se genera una traza. Inactividad : se refiere al tiempo que un env\u00edo no recibe una actualizaci\u00f3n en sus trazas.","title":"Glosario de T\u00e9rminos"},{"location":"glosario_de_terminos/#glosario-de-terminos","text":"","title":"Glosario de T\u00e9rminos"},{"location":"glosario_de_terminos/#transporte-y-distribucion","text":"TMS : sistema operativo que se utiliza dependiendo la unidad de negocio. Para \"B2C\" se utiliza INTEGRA y para \"B2B\" se utiliza ALERTRAN. B2C : abreviatura de business to consumer (negocio a consumidor final). Tambi\u00e9n incluye C2C donde un consumidor final le realiza un env\u00edo a otro consumidor final. Cuando decimos consumidor final nos referimos a una persona f\u00edsica. Hace referencia generalmente a toda la parte de \"correo\" donde se transmiten env\u00edos a nivel de \"bulto\" entre ANDREANI y las personas f\u00edsicas B2B : abreviatura de business to business (negocio a negocio). Hace referencia generalmente a toda la parte de \"log\u00edstica\" donde se transmiten env\u00edos a nivel de \"pallet\" entre clientes y ANDREANI o mismo entre plantas internas de ANDREANI Expedici\u00f3n : para el \u00e1mbito \"B2C\" se utiliza cuando un env\u00edo sale desde una planta/sucursal ANDREANI hacia otra planta/sucursal de distribuci\u00f3n. Para el \u00e1mbito \"B2B\" se denomina expedici\u00f3n a todo env\u00edo existente en el TMS donde es un conjunto de bultos que se transmiten entre plantas o centros de distribuci\u00f3n desde o para ANDREANI y sus clientes. Recepci\u00f3n : para el \u00e1mbito \"B2C\" se utiliza cuando una sucursal de distribuci\u00f3n recibe un env\u00edo proveniente de una planta o centro de distribuci\u00f3n para ser clasificado y entrar en fase de distribuci\u00f3n o custodia (seg\u00fan contrato). Pallet/Colectora/Caja/Bolsa : contenedores de bultos Aforo : medidas y peso de un bulto realizadas por balanza o sorter Custodia : para el \u00e1mbito \"B2C\" el ciclo de custodia se inicia cuando un env\u00edo llega a una sucursal y no debe distribuirse al domicilio. Puede ser por 2 motivos: por contrato (nunca se distribuye) o porque se agotaron las instancias de visitas (hasta 3 seg\u00fan contrato) Traza : cualquier env\u00edo, ya sea en el \u00e1mbito \"B2C\" o \"B2B\", va sufriendo distintas actualizaciones de estado desde que nace hasta que muere (se entrega. se devuelve al cliente, etc). Por cada cambio de estado se genera una traza. Inactividad : se refiere al tiempo que un env\u00edo no recibe una actualizaci\u00f3n en sus trazas.","title":"Transporte y Distribuci\u00f3n"},{"location":"link_interes/","text":"Links de Interes \u00b6 Visualizacion \u00b6 https://datavizcatalogue.com/ https://www.data-to-viz.com/ https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html https://colorbrewer2.org/ Ciencia de Datos \u00b6 https://medium.com/datos-y-ciencia/demasiados-l%C3%ADderes-en-ciencia-de-datos-63cc69af4457 https://towardsdatascience.com/master-sql-fundamentals-effortlessly-as-a-pandas-user-f2159c3f9bfe+ Kafka \u00b6 https://architecture-it.github.io/amqstreams-implementation SAP \u00b6 Introducci\u00f3n a la b\u00fasqueda de tablas en SAP https://grupologisticoandreani-my.sharepoint.com/:v:/g/personal/ltapia_andreani_com/EVfWWUm-b6NGg8cE9-Xe_rIBeJjtbg0jPUaORiUWK1NVbw HANA Studio \u00b6 https://grupologisticoandreani-my.sharepoint.com/:v:/g/personal/pcenturion_andreani_com/EW60husmzdZHvT4gvJtGQNEBuHAFXKmB5ScCRrJrRlj_aQ https://www.sapdatasheet.org/abap/tabl/","title":"Links de interes"},{"location":"link_interes/#links-de-interes","text":"","title":"Links de Interes"},{"location":"link_interes/#visualizacion","text":"https://datavizcatalogue.com/ https://www.data-to-viz.com/ https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html https://colorbrewer2.org/","title":"Visualizacion"},{"location":"link_interes/#ciencia-de-datos","text":"https://medium.com/datos-y-ciencia/demasiados-l%C3%ADderes-en-ciencia-de-datos-63cc69af4457 https://towardsdatascience.com/master-sql-fundamentals-effortlessly-as-a-pandas-user-f2159c3f9bfe+","title":"Ciencia de Datos"},{"location":"link_interes/#kafka","text":"https://architecture-it.github.io/amqstreams-implementation","title":"Kafka"},{"location":"link_interes/#sap","text":"Introducci\u00f3n a la b\u00fasqueda de tablas en SAP https://grupologisticoandreani-my.sharepoint.com/:v:/g/personal/ltapia_andreani_com/EVfWWUm-b6NGg8cE9-Xe_rIBeJjtbg0jPUaORiUWK1NVbw","title":"SAP"},{"location":"link_interes/#hana-studio","text":"https://grupologisticoandreani-my.sharepoint.com/:v:/g/personal/pcenturion_andreani_com/EW60husmzdZHvT4gvJtGQNEBuHAFXKmB5ScCRrJrRlj_aQ https://www.sapdatasheet.org/abap/tabl/","title":"HANA Studio"},{"location":"overview_Sonarqube/","text":"\u00b6 SonarQube es una herramienta que permite realizar un an\u00e1lisis est\u00e1tico de c\u00f3digo. Lo que hace es identificar los puntos susceptibles de mejora, que facilitar\u00e1n la obtenci\u00f3n de m\u00e9tricas necesarias para la optimizaci\u00f3n del c\u00f3digo.","title":"Overview"},{"location":"overview_Sonarqube/#_1","text":"SonarQube es una herramienta que permite realizar un an\u00e1lisis est\u00e1tico de c\u00f3digo. Lo que hace es identificar los puntos susceptibles de mejora, que facilitar\u00e1n la obtenci\u00f3n de m\u00e9tricas necesarias para la optimizaci\u00f3n del c\u00f3digo.","title":""},{"location":"overview_airflow/","text":"\u00b6 Apache Airflow es una herramienta para gestionar, monitorizar y planificar flujos de trabajo, usado como orquestador de servicios. Airflow se usa para automatizar trabajos program\u00e1ticamente dividi\u00e9ndolos en subtareas. Permite su planificaci\u00f3n y monitorizaci\u00f3n desde una herramienta centralizada. Los casos de uso m\u00e1s comunes son la automatizaci\u00f3n de ingestas de datos, acciones de mantenimiento peri\u00f3dicas y tareas de administraci\u00f3n. Para ello, permite planificar trabajos como un cron y tambi\u00e9n ejecutarlos bajo demanda.","title":"Overview"},{"location":"overview_airflow/#_1","text":"Apache Airflow es una herramienta para gestionar, monitorizar y planificar flujos de trabajo, usado como orquestador de servicios. Airflow se usa para automatizar trabajos program\u00e1ticamente dividi\u00e9ndolos en subtareas. Permite su planificaci\u00f3n y monitorizaci\u00f3n desde una herramienta centralizada. Los casos de uso m\u00e1s comunes son la automatizaci\u00f3n de ingestas de datos, acciones de mantenimiento peri\u00f3dicas y tareas de administraci\u00f3n. Para ello, permite planificar trabajos como un cron y tambi\u00e9n ejecutarlos bajo demanda.","title":""},{"location":"overview_architecture/","text":"Arquitectura Data Analysis \u00b6 El objetivo del presente documento es ser una gu\u00eda para el dise\u00f1o de los tres tipos de arquitectura (data warehouse - data lake - data lakehouse), detallando una serie de est\u00e1ndares b\u00e1sicos de modelado para tener encuenta al momento de realizar una definici\u00f3n/especificaci\u00f3n de una necesidad. Cualquier requerimiento o proyecto debe seguir los lineamientos generales que en este documento se exponen.","title":"**Arquitectura Data Analysis**"},{"location":"overview_architecture/#arquitectura-data-analysis","text":"El objetivo del presente documento es ser una gu\u00eda para el dise\u00f1o de los tres tipos de arquitectura (data warehouse - data lake - data lakehouse), detallando una serie de est\u00e1ndares b\u00e1sicos de modelado para tener encuenta al momento de realizar una definici\u00f3n/especificaci\u00f3n de una necesidad. Cualquier requerimiento o proyecto debe seguir los lineamientos generales que en este documento se exponen.","title":"Arquitectura Data Analysis"},{"location":"overview_databricks/","text":"\u00b6 Es una plataforma de an\u00e1lisis de datos optimizada para la plataforma de servicios en la nube de Microsoft Azure. Azure Databricks ofrece tres entornos para desarrollar aplicaciones que consumen muchos datos: Databricks SQL, Databricks Data Science & Engineering y Databricks Machine Learning.","title":"Overview"},{"location":"overview_databricks/#_1","text":"Es una plataforma de an\u00e1lisis de datos optimizada para la plataforma de servicios en la nube de Microsoft Azure. Azure Databricks ofrece tres entornos para desarrollar aplicaciones que consumen muchos datos: Databricks SQL, Databricks Data Science & Engineering y Databricks Machine Learning.","title":""},{"location":"overview_deequ/","text":"Deequ \u00b6 \u00bfQu\u00e9 es ? \u00b6 Es un marco de c\u00f3digo abierto para probar la calidad de los datos. Est\u00e1 dise\u00f1ado para escalar a grandes conjuntos de datos. El sistema calcula las m\u00e9tricas de calidad de los datos con regularidad, verifica las restricciones definidas y publica conjuntos de datos en caso de \u00e9xito. Nos Permite \u00b6 Calcular m\u00e9tricas de calidad de datos en nuestro conjunto de datos Definir y verificar las limitaciones de calidad de los datos. Especificar qu\u00e9 comprobaciones de restricciones se realizar\u00e1n en sus datos. Sugerir las limitaciones de calidad de los datos en las tables/archivos de entrada Verificar las restricciones sugeridas antes mencionadas Calcular las m\u00e9tricas de calidad Ejecutar la elaboraci\u00f3n de perfiles de columna 3 Componentes Importantes \u00b6 Constraint Suggestion : podemos definir nuestras propias restricciones de calidad de datos personalizadas o utilizar los m\u00e9todos automatizados de sugerencia de restricci\u00f3n que perfilan los datos para inferir restricciones \u00fatiles. Constraint Verification : como usuarios, nos enfocamos en definir un conjunto de restricciones de calidad de datos que se deben verificar. Deequ se encarga de derivar el conjunto requerido de m\u00e9tricas que se calcular\u00e1n sobre los datos. Deequ genera un informe de calidad de datos, que contiene el resultado de la verificaci\u00f3n de la restricci\u00f3n. Metrics Computation : Deequ puede calcular m\u00e9tricas de calidad de datos con regularidad.","title":"Overview"},{"location":"overview_deequ/#deequ","text":"","title":"Deequ"},{"location":"overview_deequ/#que-es","text":"Es un marco de c\u00f3digo abierto para probar la calidad de los datos. Est\u00e1 dise\u00f1ado para escalar a grandes conjuntos de datos. El sistema calcula las m\u00e9tricas de calidad de los datos con regularidad, verifica las restricciones definidas y publica conjuntos de datos en caso de \u00e9xito.","title":"\u00bfQu\u00e9 es ?"},{"location":"overview_deequ/#nos-permite","text":"Calcular m\u00e9tricas de calidad de datos en nuestro conjunto de datos Definir y verificar las limitaciones de calidad de los datos. Especificar qu\u00e9 comprobaciones de restricciones se realizar\u00e1n en sus datos. Sugerir las limitaciones de calidad de los datos en las tables/archivos de entrada Verificar las restricciones sugeridas antes mencionadas Calcular las m\u00e9tricas de calidad Ejecutar la elaboraci\u00f3n de perfiles de columna","title":"Nos Permite"},{"location":"overview_deequ/#3-componentes-importantes","text":"Constraint Suggestion : podemos definir nuestras propias restricciones de calidad de datos personalizadas o utilizar los m\u00e9todos automatizados de sugerencia de restricci\u00f3n que perfilan los datos para inferir restricciones \u00fatiles. Constraint Verification : como usuarios, nos enfocamos en definir un conjunto de restricciones de calidad de datos que se deben verificar. Deequ se encarga de derivar el conjunto requerido de m\u00e9tricas que se calcular\u00e1n sobre los datos. Deequ genera un informe de calidad de datos, que contiene el resultado de la verificaci\u00f3n de la restricci\u00f3n. Metrics Computation : Deequ puede calcular m\u00e9tricas de calidad de datos con regularidad.","title":"3 Componentes Importantes"},{"location":"overview_devops/","text":"","title":"Overview devops"},{"location":"overview_gobierno_del_dato/","text":"Data Governance \u00b6 Cuando hablamos de Data Governance nos referimos, ante todo, a una funci\u00f3n de la gesti\u00f3n de datos para garantizar la calidad, integridad, seguridad y usabilidad de los datos recopilados por una organizaci\u00f3n y debe abarcar todo el ciclo de vida de estos. Estas pr\u00e1cticas se configuran como est\u00e1ndares y son una intersecci\u00f3n entre normas de privacidad y reglamentos internos de las empresas. \u00bfQu\u00e9 garantiza el gobierno de datos? \u00b6 Nos aseguramos de que los datos: : Sean correctos Est\u00e9n actualizados y sean coherentes Sean seguros: Solo los usuarios permitidos tengan acceso Sea auditable (registro de accesos y cambios) Se cumplan con regulaciones \u00bfCu\u00e1l es su prop\u00f3sito? \u00b6 El gobierno de datos tiene como principal finalidad mejorar la confianza en los datos. Que los datos sean confiables son una condici\u00f3n necesaria para emplear los datos en la toma de decisiones. \u00bfQu\u00e9 aspectos son clave para la gobernanza de datos? \u00b6 Discoverability : metadata management, data quality, master data management Security : data privacy, data security Accountability : Domain based governance \u00bfPor qu\u00e9 la gobernanza de datos es cada vez m\u00e1s importante? \u00b6 Tradicionalmente el gobierno de datos se consider\u00f3 solo como una funci\u00f3n de TI, sin embargo, dada la envergadura de la revoluci\u00f3n digital, el big data y las diferentes regulaciones introducidas en industrias, hacen del Data Governance un eje principal a la hora de gestionar la data adecuadamente. \u00bfQu\u00e9 cambi\u00f3? \u00b6 Los 7 eventos que nos obligan a implementar Data Governance: \u00b6 Se generan much\u00edsimos datos Aumento en la cantidad de personas que trabajan con datos M\u00e9todos de captura (data points) cada vez m\u00e1s avanzados Recolecci\u00f3n de datos no estructurados Aumento de casos de uso en la gesti\u00f3n de datos Nuevas regulaciones y leyes Preocupaci\u00f3n \u00e9tica en el uso de datos \u00bfY cu\u00e1les son las 10 principales acciones de implementaci\u00f3n? \u00b6 Diccionario empresarial: clases, pol\u00edticas y casos de uso Clasificaci\u00f3n de datos y organizaci\u00f3n Catalogaci\u00f3n de datos y gesti\u00f3n de metadatos Calidad de datos Trazabilidad y linaje Privacidad y cifrado Persistencia y eliminaci\u00f3n de datos Workflow de adquisici\u00f3n de datos IAM: administraci\u00f3n de identidad y acceso Autorizaci\u00f3n de usuario y gesti\u00f3n de accesos","title":"Overview"},{"location":"overview_gobierno_del_dato/#data-governance","text":"Cuando hablamos de Data Governance nos referimos, ante todo, a una funci\u00f3n de la gesti\u00f3n de datos para garantizar la calidad, integridad, seguridad y usabilidad de los datos recopilados por una organizaci\u00f3n y debe abarcar todo el ciclo de vida de estos. Estas pr\u00e1cticas se configuran como est\u00e1ndares y son una intersecci\u00f3n entre normas de privacidad y reglamentos internos de las empresas.","title":"Data Governance"},{"location":"overview_gobierno_del_dato/#que-garantiza-el-gobierno-de-datos","text":"Nos aseguramos de que los datos: : Sean correctos Est\u00e9n actualizados y sean coherentes Sean seguros: Solo los usuarios permitidos tengan acceso Sea auditable (registro de accesos y cambios) Se cumplan con regulaciones","title":"\u00bfQu\u00e9 garantiza el gobierno de datos?"},{"location":"overview_gobierno_del_dato/#cual-es-su-proposito","text":"El gobierno de datos tiene como principal finalidad mejorar la confianza en los datos. Que los datos sean confiables son una condici\u00f3n necesaria para emplear los datos en la toma de decisiones.","title":"\u00bfCu\u00e1l es su prop\u00f3sito?"},{"location":"overview_gobierno_del_dato/#que-aspectos-son-clave-para-la-gobernanza-de-datos","text":"Discoverability : metadata management, data quality, master data management Security : data privacy, data security Accountability : Domain based governance","title":"\u00bfQu\u00e9 aspectos son clave para la gobernanza de datos?"},{"location":"overview_gobierno_del_dato/#por-que-la-gobernanza-de-datos-es-cada-vez-mas-importante","text":"Tradicionalmente el gobierno de datos se consider\u00f3 solo como una funci\u00f3n de TI, sin embargo, dada la envergadura de la revoluci\u00f3n digital, el big data y las diferentes regulaciones introducidas en industrias, hacen del Data Governance un eje principal a la hora de gestionar la data adecuadamente.","title":"\u00bfPor qu\u00e9 la gobernanza de datos es cada vez m\u00e1s importante?"},{"location":"overview_gobierno_del_dato/#que-cambio","text":"","title":"\u00bfQu\u00e9 cambi\u00f3?"},{"location":"overview_gobierno_del_dato/#los-7-eventos-que-nos-obligan-a-implementar-data-governance","text":"Se generan much\u00edsimos datos Aumento en la cantidad de personas que trabajan con datos M\u00e9todos de captura (data points) cada vez m\u00e1s avanzados Recolecci\u00f3n de datos no estructurados Aumento de casos de uso en la gesti\u00f3n de datos Nuevas regulaciones y leyes Preocupaci\u00f3n \u00e9tica en el uso de datos","title":"Los 7 eventos que nos obligan a implementar Data Governance:"},{"location":"overview_gobierno_del_dato/#y-cuales-son-las-10-principales-acciones-de-implementacion","text":"Diccionario empresarial: clases, pol\u00edticas y casos de uso Clasificaci\u00f3n de datos y organizaci\u00f3n Catalogaci\u00f3n de datos y gesti\u00f3n de metadatos Calidad de datos Trazabilidad y linaje Privacidad y cifrado Persistencia y eliminaci\u00f3n de datos Workflow de adquisici\u00f3n de datos IAM: administraci\u00f3n de identidad y acceso Autorizaci\u00f3n de usuario y gesti\u00f3n de accesos","title":"\u00bfY cu\u00e1les son las 10 principales acciones de implementaci\u00f3n?"},{"location":"purview/","text":"\u00b6 Azure Purview es un servicio de gobierno de datos unificado que nos ayuda a administrar y controlar sus datos locales, de varias nubes y de software como servicio (SaaS). Nos permite crear mapa hol\u00edstico y actualizado de nuestro entorno de datos con descubrimiento de datos automatizado, clasificaci\u00f3n de datos confidenciales y linaje de datos de extremo a extremo. Los consumidores de datos van a encontrar valiosos y confiables. Arquitectura de alto nivel de Azure Purview, que muestra fuentes locales y m\u00faltiples nubes que fluyen hacia Azure Purview, y las aplicaciones de Azure Purview (Data Catalog, Map e Insights) que permiten a los consumidores y curadores de datos ver y administrar metadatos. Estos metadatos tambi\u00e9n se extender\u00e1n a servicios de an\u00e1lisis externos de Azure Purview para su procesamiento posterior. App Descripci\u00f3n Data Map Hace que los datos sean significativos al graficar los activos de datos y sus relaciones en todo nuestro patrimonio de datos. El mapa de datos utilizado para descubrir datos y administrar el acceso a esos datos. Data Catalog Encuentra fuentes de datos confiables navegando y buscando activos de datos. El cat\u00e1logo de datos alinea activos con t\u00e9rminos comerciales amigables y clasificaci\u00f3n de datos para identificar fuentes de datos. Data Insights Brinda una descripci\u00f3n general de suestado de datos para ayudar a descubrir qu\u00e9 tipo de datos tiene y d\u00f3nde. Ventajas de Azure Purview \u00b6 Azure Purview nos ayuda a obtener el m\u00e1ximo valor de nuestros activos de informaci\u00f3n existentes. El cat\u00e1logo hace que las fuentes de datos sean f\u00e1cilmente detectables y comprensibles para los usuarios que gestionan los datos. Azure Purview proporciona un servicio basado en la nube en el que puede registrar fuentes de datos. Durante el registro, los datos permanecen en su ubicaci\u00f3n actual, pero se agrega una copia de sus metadatos a Azure Purview, junto con una referencia a la ubicaci\u00f3n del origen de datos. Los metadatos tambi\u00e9n se indexan para que cada fuente de datos sea f\u00e1cilmente detectable a trav\u00e9s de la b\u00fasqueda y comprensible para los usuarios que la descubren. Despu\u00e9s de registrar una fuente de datos, podemos enriquecer los metadatos. El usuario que registr\u00f3 la fuente de datos u otro usuario de la empresa agrega los metadatos. Cualquier usuario puede anotar una fuente de datos proporcionando descripciones, etiquetas u otros metadatos para solicitar acceso a la fuente de datos. Estos metadatos descriptivos complementan los metadatos estructurales, como nombres de columnas y tipos de datos, que se registran desde el origen de datos. Descubrir y comprender las fuentes de datos y su uso es nuestro objetivo principal del registro de las fuentes. Los usuarios empresariales pueden necesitar datos para inteligencia comercial, desarrollo de aplicaciones, ciencia de datos o cualquier otra tarea en la que se requieran los datos correctos. Usan la experiencia de descubrimiento del cat\u00e1logo de datos para encontrar r\u00e1pidamente datos que se ajusten a sus necesidades, comprender los datos para evaluar su idoneidad para el prop\u00f3sito y consumir los datos abriendo la fuente de datos en la herramienta de su elecci\u00f3n. Al mismo tiempo, los usuarios pueden contribuir al cat\u00e1logo etiquetando, documentando y anotando fuentes de datos que ya se han registrado. Tambi\u00e9n pueden registrar nuevas fuentes de datos, que luego son descubiertas, comprendidas y consumidas por la comunidad de usuarios del cat\u00e1logo.","title":"Purview"},{"location":"purview/#_1","text":"Azure Purview es un servicio de gobierno de datos unificado que nos ayuda a administrar y controlar sus datos locales, de varias nubes y de software como servicio (SaaS). Nos permite crear mapa hol\u00edstico y actualizado de nuestro entorno de datos con descubrimiento de datos automatizado, clasificaci\u00f3n de datos confidenciales y linaje de datos de extremo a extremo. Los consumidores de datos van a encontrar valiosos y confiables. Arquitectura de alto nivel de Azure Purview, que muestra fuentes locales y m\u00faltiples nubes que fluyen hacia Azure Purview, y las aplicaciones de Azure Purview (Data Catalog, Map e Insights) que permiten a los consumidores y curadores de datos ver y administrar metadatos. Estos metadatos tambi\u00e9n se extender\u00e1n a servicios de an\u00e1lisis externos de Azure Purview para su procesamiento posterior. App Descripci\u00f3n Data Map Hace que los datos sean significativos al graficar los activos de datos y sus relaciones en todo nuestro patrimonio de datos. El mapa de datos utilizado para descubrir datos y administrar el acceso a esos datos. Data Catalog Encuentra fuentes de datos confiables navegando y buscando activos de datos. El cat\u00e1logo de datos alinea activos con t\u00e9rminos comerciales amigables y clasificaci\u00f3n de datos para identificar fuentes de datos. Data Insights Brinda una descripci\u00f3n general de suestado de datos para ayudar a descubrir qu\u00e9 tipo de datos tiene y d\u00f3nde.","title":""},{"location":"purview/#ventajas-de-azure-purview","text":"Azure Purview nos ayuda a obtener el m\u00e1ximo valor de nuestros activos de informaci\u00f3n existentes. El cat\u00e1logo hace que las fuentes de datos sean f\u00e1cilmente detectables y comprensibles para los usuarios que gestionan los datos. Azure Purview proporciona un servicio basado en la nube en el que puede registrar fuentes de datos. Durante el registro, los datos permanecen en su ubicaci\u00f3n actual, pero se agrega una copia de sus metadatos a Azure Purview, junto con una referencia a la ubicaci\u00f3n del origen de datos. Los metadatos tambi\u00e9n se indexan para que cada fuente de datos sea f\u00e1cilmente detectable a trav\u00e9s de la b\u00fasqueda y comprensible para los usuarios que la descubren. Despu\u00e9s de registrar una fuente de datos, podemos enriquecer los metadatos. El usuario que registr\u00f3 la fuente de datos u otro usuario de la empresa agrega los metadatos. Cualquier usuario puede anotar una fuente de datos proporcionando descripciones, etiquetas u otros metadatos para solicitar acceso a la fuente de datos. Estos metadatos descriptivos complementan los metadatos estructurales, como nombres de columnas y tipos de datos, que se registran desde el origen de datos. Descubrir y comprender las fuentes de datos y su uso es nuestro objetivo principal del registro de las fuentes. Los usuarios empresariales pueden necesitar datos para inteligencia comercial, desarrollo de aplicaciones, ciencia de datos o cualquier otra tarea en la que se requieran los datos correctos. Usan la experiencia de descubrimiento del cat\u00e1logo de datos para encontrar r\u00e1pidamente datos que se ajusten a sus necesidades, comprender los datos para evaluar su idoneidad para el prop\u00f3sito y consumir los datos abriendo la fuente de datos en la herramienta de su elecci\u00f3n. Al mismo tiempo, los usuarios pueden contribuir al cat\u00e1logo etiquetando, documentando y anotando fuentes de datos que ya se han registrado. Tambi\u00e9n pueden registrar nuevas fuentes de datos, que luego son descubiertas, comprendidas y consumidas por la comunidad de usuarios del cat\u00e1logo.","title":"Ventajas de Azure Purview"},{"location":"sonarqube/","text":"\u00bfQu\u00e9 informaci\u00f3n nos puede mostrar una herramienta como SonarQube? \u00b6 Quality Gate \u00b6 Son una serie de condiciones (reglas) que el proyecto analizado debe cumplir para poder pasar a una siguiente etapa, all\u00ed b\u00e1sicamente se busca responder a la pregunta: \u00bfPuede este c\u00f3digo seguir avanzando en el ciclo de desarrollo? Bugs y vulnerabilidades \u00b6 Hacen referencia tanto a puntos de fallo reales o potenciales en el software, como a puntos d\u00e9biles de seguridad que pueden ser usados como foco de un ataque. Code smells \u00b6 Es un indicativo de que quiz\u00e1s no estamos escribiendo c\u00f3digo de la mejor manera, lo que puede ocasionar alg\u00fan problema en el futuro y un problema de trasfondo, normalmente problemas de mantenibilidad del c\u00f3digo. Lo anterior no quiere decir que el c\u00f3digo contenga errores o bugs, pero s\u00ed puede aumentar el riesgo a errores o fallos El concepto de code smells est\u00e1 muy asociado con la deuda t\u00e9cnica, esta hace referencia a la cantidad de tiempo que tardar\u00edamos en mejorar algunos detalles identificados por SonarQube. Coverage \u00b6 La cobertura de c\u00f3digo es una medida que permite conocer el porcentaje de c\u00f3digo que ha sido probado o validado por tests. Usualmente se desea obtener una mayor cobertura para tener menos probabilidad de introducir errores en el c\u00f3digo.","title":"SonarQube"},{"location":"sonarqube/#que-informacion-nos-puede-mostrar-una-herramienta-como-sonarqube","text":"","title":"\u00bfQu\u00e9 informaci\u00f3n nos puede mostrar una herramienta como SonarQube?"},{"location":"sonarqube/#quality-gate","text":"Son una serie de condiciones (reglas) que el proyecto analizado debe cumplir para poder pasar a una siguiente etapa, all\u00ed b\u00e1sicamente se busca responder a la pregunta: \u00bfPuede este c\u00f3digo seguir avanzando en el ciclo de desarrollo?","title":"Quality Gate"},{"location":"sonarqube/#bugs-y-vulnerabilidades","text":"Hacen referencia tanto a puntos de fallo reales o potenciales en el software, como a puntos d\u00e9biles de seguridad que pueden ser usados como foco de un ataque.","title":"Bugs y vulnerabilidades"},{"location":"sonarqube/#code-smells","text":"Es un indicativo de que quiz\u00e1s no estamos escribiendo c\u00f3digo de la mejor manera, lo que puede ocasionar alg\u00fan problema en el futuro y un problema de trasfondo, normalmente problemas de mantenibilidad del c\u00f3digo. Lo anterior no quiere decir que el c\u00f3digo contenga errores o bugs, pero s\u00ed puede aumentar el riesgo a errores o fallos El concepto de code smells est\u00e1 muy asociado con la deuda t\u00e9cnica, esta hace referencia a la cantidad de tiempo que tardar\u00edamos en mejorar algunos detalles identificados por SonarQube.","title":"Code smells"},{"location":"sonarqube/#coverage","text":"La cobertura de c\u00f3digo es una medida que permite conocer el porcentaje de c\u00f3digo que ha sido probado o validado por tests. Usualmente se desea obtener una mayor cobertura para tener menos probabilidad de introducir errores en el c\u00f3digo.","title":"Coverage"}]}